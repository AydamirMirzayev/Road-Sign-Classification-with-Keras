{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCode to hold and test a simple demonstration of KERAS library for traffic sign classification\\n\\nAuthors\\n-------\\nAydamir Mirzayev: https://github.com/AydamirMirzayev || https://www.linkedin.com/in/aydamir-mirzayev-97b297133/\\n\\nMethods\\n-------\\nget_data( global_path)\\n    extract and perform necessary modifications on the dataset\\n\\nsame_size( data_collection)\\n    Resize all images to the same size. Use mean size of all images\\n    \\ncategorical( labels)\\n    One hot encode the labels\\n\\nreport_accuracy(predicted, true)\\n    Report accuracy of prediction\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from skimage import exposure\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Dropout\n",
    "import time as t\n",
    "\n",
    "\"\"\"\n",
    "Code to hold and test a simple demonstration of KERAS library for traffic sign classification\n",
    "\n",
    "Authors\n",
    "-------\n",
    "Aydamir Mirzayev: https://github.com/AydamirMirzayev || https://www.linkedin.com/in/aydamir-mirzayev-97b297133/\n",
    "\n",
    "Methods\n",
    "-------\n",
    "get_data( global_path)\n",
    "    extract and perform necessary modifications on the dataset\n",
    "\n",
    "same_size( data_collection)\n",
    "    Resize all images to the same size. Use mean size of all images\n",
    "    \n",
    "categorical( labels)\n",
    "    One hot encode the labels\n",
    "\n",
    "report_accuracy(predicted, true)\n",
    "    Report accuracy of prediction\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data( global_path = \"D:\\D_Desktop\\Courses\\PAST\\ML 550\\project\\data2\\gtsrb-german-traffic-sign\"):\n",
    "    \"\"\" Extract the data, merge test and training samples, find class distribution, and train/test/e\n",
    "        \n",
    "    Parameters:\n",
    "    global_path (int): Path to the gtsrb-german-traffic-sign folder\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    labels                   (List): labels of the clusters on which clustering has been performed\n",
    "    data_collection          (List): image collection\n",
    "    used_classes            (Array): set of images\n",
    "    general_c_freq      (int array): frequency of each class\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    # Import both test and train data labels\n",
    "    test_path = global_path + '/Test.csv'\n",
    "    test_labels = np.array( pd.read_csv( test_path))\n",
    "    train_path = global_path + '/Train.csv'\n",
    "    train_labels = np.array( pd.read_csv( train_path))\n",
    "\n",
    "    #Extract train class labels from general labels and count unique classes\n",
    "    train_class_labels = train_labels[:,6]\n",
    "    train_unique = np.unique( train_class_labels)\n",
    "    train_c_freq = np.zeros( train_unique.shape[0])\n",
    "    \n",
    "    #calculate train class frequency \n",
    "    for i,u in enumerate( train_unique):\n",
    "        train_c_freq[i] = sum( train_class_labels == u)\n",
    "\n",
    "    #Extract test class labels from general labels and count unique classes\n",
    "    test_class_labels = test_labels[:,6]\n",
    "    test_unique = np.unique( test_class_labels)\n",
    "    test_c_freq = np.zeros( test_unique.shape[0])\n",
    "    \n",
    "    #calculate test class frequency \n",
    "    for i,u in enumerate( test_unique):\n",
    "        test_c_freq[i] = sum( test_class_labels == u)\n",
    "    \n",
    "    #calculate total class frequency and find used classes\n",
    "    general_c_freq = train_c_freq + test_c_freq\n",
    "    used_classes = test_unique[ np.argsort( general_c_freq)][35:]\n",
    "\n",
    "    # load train images normalize  and flatten\n",
    "    train_collection = []\n",
    "    new_train_labels = []\n",
    "    for c in used_classes:\n",
    "        names = train_labels[ train_labels[ :, 6] == c][:, 7]\n",
    "        new_train_labels.append( train_labels [ train_labels[ :, 6] == c])\n",
    "        for name in names:\n",
    "            path = global_path + '/' + name \n",
    "            train_collection.append( mpimg.imread( path))\n",
    "\n",
    "    new_flat_train_labels = []\n",
    "    for i in range( len(new_train_labels)):\n",
    "        for j in range( len(new_train_labels[i])):\n",
    "            new_flat_train_labels.append(new_train_labels[i][j])\n",
    "\n",
    "    # load test images, normalize and flatten\n",
    "    test_collection = []\n",
    "    new_test_labels = []\n",
    "    for c in used_classes:\n",
    "        names = test_labels [test_labels[ :, 6] == c][:,7] \n",
    "        new_test_labels.append(test_labels [test_labels[ :, 6] == c])\n",
    "        for name in names:\n",
    "            path = global_path + '/' + name \n",
    "            test_collection.append( mpimg.imread( path))\n",
    "\n",
    "    new_flat_test_labels = []\n",
    "    for i in range( len(new_test_labels)):\n",
    "        for j in range( len(new_test_labels[i])):\n",
    "            new_flat_test_labels.append(new_test_labels[i][j])\n",
    "\n",
    "    # generate general labels and data\n",
    "    labels = []\n",
    "    data_collection = []\n",
    "\n",
    "    for i in range( len(test_collection)):\n",
    "        data_collection.append( test_collection[i])\n",
    "        labels.append( new_flat_test_labels[i])\n",
    "\n",
    "    for i in range( len( train_collection)):\n",
    "        data_collection.append( train_collection[i])\n",
    "        labels.append( new_flat_train_labels[i])\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    #shuffle several times\n",
    "    for i in range(10):\n",
    "        labels, data_collection = shuffle( labels, data_collection)\n",
    "        \n",
    "    return labels, data_collection, used_classes, general_c_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_size( data_collection):\n",
    "    \"\"\" Resize all images to the same size. Use mean size of all images\n",
    "        \n",
    "    Parameters:\n",
    "    data_collection (List): list of all images\n",
    "    \n",
    "    Returns:\n",
    "    new_data_collection   (List): normalized list of images\n",
    "    \"\"\"   \n",
    "    \n",
    "    #calculate the image size mean\n",
    "    h_mean = 0\n",
    "    w_mean = 0\n",
    "    \n",
    "    for a in data_collection:\n",
    "        h_mean += a.shape[0]\n",
    "        w_mean += a.shape[1]\n",
    "    \n",
    "    #round up the mean\n",
    "    w_mean = round( w_mean/len( data_collection))\n",
    "    h_mean = round( h_mean/len( data_collection))\n",
    "\n",
    "    #resize all the images\n",
    "    new_data_collection = []\n",
    "    for i, a in enumerate( data_collection):\n",
    "        resized = resize( a, (h_mean, w_mean), anti_aliasing=True)\n",
    "        normalized = exposure.rescale_intensity( resized)\n",
    "        new_data_collection.append( normalized)\n",
    "    \n",
    "    return new_data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical( labels):\n",
    "    \"\"\" One hot encode the labels\n",
    "        \n",
    "    Parameters:\n",
    "    labels (List): list of all image labels\n",
    "    \n",
    "    Returns:\n",
    "    new_classes   (List): one hot encoded labels\n",
    "    \"\"\"   \n",
    "    \n",
    "    classes = np.array( labels)[:,6] #extract class info\n",
    "\n",
    "    new_classes = np.zeros( np.array( labels).shape[0])\n",
    "    \n",
    "    for i, a in enumerate( np.unique( classes)): #categorize\n",
    "        new_classes[ classes == a ] = i\n",
    "    \n",
    "    new_classes = to_categorical( new_classes)\n",
    "    return new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_accuracy(predicted, true):\n",
    "    \"\"\" Report accuracy of prediction\n",
    "        \n",
    "    Parameters:\n",
    "    predicted  (int array): array of predicted labels\n",
    "    true       (int array): array of true labels\n",
    "    \n",
    "    Returns:\n",
    "    new_classes   (List): one hot encoded labels\n",
    "    \"\"\"   \n",
    "    \n",
    "    #calculate accuracy\n",
    "    acc = sum( np.argmax( predicted, 1) == np.argmax( true, 1) ) / predicted.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data\n",
    "labels, data_collection, used_classes, general_c_freq = get_data()\n",
    "# resize image to the mean size of the collection\n",
    "data_collection = same_size(data_collection)\n",
    "# one hot encode the labels\n",
    "class_labels = categorical( labels)\n",
    "\n",
    "# Train, validation, and test split the dataset\n",
    "X_train, X_test, y_train, y_test  = train_test_split( np.array(data_collection), class_labels, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split( X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #define a sequential model\n",
    "\n",
    "# define convolutional layers\n",
    "model.add( Conv2D( 32, kernel_size=3, activation= 'relu', input_shape=( 49, 49, 3))) \n",
    "model.add( MaxPooling2D(pool_size=2, strides = 2))\n",
    "model.add( Conv2D( 16, kernel_size=5, activation= 'relu'))\n",
    "model.add( MaxPooling2D(pool_size=2, strides = 2))\n",
    "model.add( Conv2D( 8, kernel_size=5, activation= 'relu'))\n",
    "model.add( MaxPooling2D(pool_size=2, strides = 2))\n",
    "\n",
    "# define fully connected (dense) layer\n",
    "model.add( Flatten())\n",
    "model.add( Dense(8, activation= 'softmax' ))\n",
    "\n",
    "# define optimization function\n",
    "sgd = keras.optimizers.SGD(learning_rate=0.02, momentum=0.2, nesterov= True)\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer= sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the network measure running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17736 samples, validate on 2217 samples\n",
      "Epoch 1/2\n",
      "17736/17736 [==============================] - 62s 4ms/step - loss: 0.9598 - accuracy: 0.6629 - val_loss: 0.2974 - val_accuracy: 0.9215\n",
      "Epoch 2/2\n",
      "17736/17736 [==============================] - 61s 3ms/step - loss: 0.2085 - accuracy: 0.9429 - val_loss: 0.1457 - val_accuracy: 0.9617\n",
      "Training Time:  123.50570034980774\n"
     ]
    }
   ],
   "source": [
    "start = t.time()\n",
    "model.fit( X_train , y_train, validation_data=(X_val, y_val), epochs= 2)\n",
    "end = t.time()\n",
    "print('Training Time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Time:  2.5534069538116455\n",
      "Accuracy:  97.0 %\n"
     ]
    }
   ],
   "source": [
    "start = t.time()\n",
    "predicted = model.predict(X_test)\n",
    "end = t.time()\n",
    "print('Prediction Time: ', end - start)\n",
    "print('Accuracy: ',  round(report_accuracy(predicted,y_test),2)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
